{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf5302fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_manager.py\n",
    "import tushare as ts\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class TushareDataManager:\n",
    "    def __init__(self, token, cache_dir='./tushare_cache'):\n",
    "        self.pro = ts.pro_api(token)\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        print(f\"TushareDataManager initialized. Cache directory: {self.cache_dir}\")\n",
    "\n",
    "    def _get_cache_path(self, data_type, ts_code=None, start_date=None, end_date=None):\n",
    "        if data_type == 'index_weekly':\n",
    "            return os.path.join(self.cache_dir, f'index_weekly_{ts_code}.parquet')\n",
    "        elif data_type == 'index_constituents':\n",
    "            return os.path.join(self.cache_dir, f'index_constituents_{ts_code}.parquet')\n",
    "        elif data_type == 'daily_basic':\n",
    "            return os.path.join(self.cache_dir, f'daily_basic_{ts_code}.parquet')\n",
    "        elif data_type == 'index_daily': # For CSI300 benchmark\n",
    "            return os.path.join(self.cache_dir, f'index_daily_{ts_code}.parquet')\n",
    "        return None\n",
    "\n",
    "    def _load_from_cache(self, file_path, start_date_str, end_date_str):\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path)\n",
    "                df['trade_date'] = pd.to_datetime(df['trade_date'])\n",
    "                \n",
    "                # Check if the cached data covers the requested range\n",
    "                if not df.empty and \\\n",
    "                   df['trade_date'].min() <= pd.to_datetime(start_date_str) and \\\n",
    "                   df['trade_date'].max() >= pd.to_datetime(end_date_str):\n",
    "                    print(f\"Loaded from cache: {file_path}\")\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading from cache {file_path}: {e}. Re-fetching.\")\n",
    "        return None\n",
    "\n",
    "    def _save_to_cache(self, df, file_path):\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        df['trade_date'] = df['trade_date'].dt.strftime('%Y%m%d') # Store as string\n",
    "        df.to_parquet(file_path, index=False)\n",
    "        print(f\"Saved to cache: {file_path}\")\n",
    "\n",
    "    def get_index_weekly_data(self, ts_code, start_date, end_date):\n",
    "        start_date_str = start_date.strftime('%Y%m%d')\n",
    "        end_date_str = end_date.strftime('%Y%m%d')\n",
    "        file_path = self._get_cache_path('index_weekly', ts_code)\n",
    "\n",
    "        cached_df = self._load_from_cache(file_path, start_date_str, end_date_str)\n",
    "        if cached_df is not None:\n",
    "            return cached_df[(cached_df['trade_date'] >= start_date) & (cached_df['trade_date'] <= end_date)]\n",
    "\n",
    "        print(f\"Fetching weekly data for {ts_code} from Tushare...\")\n",
    "        df = self.pro.pro_bar(ts_code=ts_code, asset='I', freq='W',\n",
    "                              start_date=start_date_str, end_date=end_date_str,\n",
    "                              fields='trade_date,close,open,high,low,vol,amount')\n",
    "        if df is None or df.empty:\n",
    "            print(f\"No data fetched for {ts_code} in range {start_date_str} to {end_date_str}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df['trade_date'] = pd.to_datetime(df['trade_date'])\n",
    "        df = df.sort_values('trade_date').reset_index(drop=True)\n",
    "        self._save_to_cache(df, file_path)\n",
    "        return df\n",
    "\n",
    "    def get_index_daily_data(self, ts_code, start_date, end_date):\n",
    "        start_date_str = start_date.strftime('%Y%m%d')\n",
    "        end_date_str = end_date.strftime('%Y%m%d')\n",
    "        file_path = self._get_cache_path('index_daily', ts_code)\n",
    "\n",
    "        cached_df = self._load_from_cache(file_path, start_date_str, end_date_str)\n",
    "        if cached_df is not None:\n",
    "            return cached_df[(cached_df['trade_date'] >= start_date) & (cached_df['trade_date'] <= end_date)]\n",
    "\n",
    "        print(f\"Fetching daily data for {ts_code} from Tushare...\")\n",
    "        df = self.pro.index_daily(ts_code=ts_code,\n",
    "                                  start_date=start_date_str, end_date=end_date_str,\n",
    "                                  fields='trade_date,close,open,high,low,vol,amount')\n",
    "        if df is None or df.empty:\n",
    "            print(f\"No data fetched for {ts_code} in range {start_date_str} to {end_date_str}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df['trade_date'] = pd.to_datetime(df['trade_date'])\n",
    "        df = df.sort_values('trade_date').reset_index(drop=True)\n",
    "        self._save_to_cache(df, file_path)\n",
    "        return df\n",
    "\n",
    "    def get_index_constituents(self, index_code, trade_date):\n",
    "        # index_weight is monthly, so we need to find the closest month-end or month-start\n",
    "        # For simplicity, let's try to get data for the first day of the month\n",
    "        # or the last day of the previous month if current month's data is not available.\n",
    "        # Tushare's index_weight 'trade_date' is usually the last trading day of the month.\n",
    "        \n",
    "        # Try current month's last day\n",
    "        current_month_end = trade_date.replace(day=1) + timedelta(days=31)\n",
    "        current_month_end = current_month_end.replace(day=1) - timedelta(days=1)\n",
    "        \n",
    "        # Try previous month's last day\n",
    "        prev_month_end = trade_date.replace(day=1) - timedelta(days=1)\n",
    "\n",
    "        dates_to_try = [current_month_end, prev_month_end]\n",
    "        \n",
    "        for d in dates_to_try:\n",
    "            date_str = d.strftime('%Y%m%d')\n",
    "            file_path = self._get_cache_path('index_constituents', index_code)\n",
    "            \n",
    "            # Load all cached constituents for this index and filter by date\n",
    "            cached_df = self._load_from_cache(file_path, '19900101', '20991231') # Load all to check specific date\n",
    "            if cached_df is not None and not cached_df.empty:\n",
    "                specific_date_df = cached_df[cached_df['trade_date'] == d]\n",
    "                if not specific_date_df.empty:\n",
    "                    print(f\"Loaded constituents for {index_code} on {date_str} from cache.\")\n",
    "                    return specific_date_df\n",
    "            \n",
    "            print(f\"Fetching constituents for {index_code} on {date_str} from Tushare...\")\n",
    "            df = self.pro.index_weight(index_code=index_code, trade_date=date_str)\n",
    "            if df is not None and not df.empty:\n",
    "                df['trade_date'] = pd.to_datetime(df['trade_date'])\n",
    "                # Append to existing cache or create new\n",
    "                if cached_df is not None and not cached_df.empty:\n",
    "                    combined_df = pd.concat([cached_df, df]).drop_duplicates(subset=['index_code', 'con_code', 'trade_date']).reset_index(drop=True)\n",
    "                    self._save_to_cache(combined_df, file_path)\n",
    "                else:\n",
    "                    self._save_to_cache(df, file_path)\n",
    "                return df\n",
    "        \n",
    "        print(f\"No constituents data found for {index_code} around {trade_date.strftime('%Y%m%d')}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def get_daily_basic(self, ts_code, trade_date):\n",
    "        date_str = trade_date.strftime('%Y%m%d')\n",
    "        file_path = self._get_cache_path('daily_basic', ts_code)\n",
    "        \n",
    "        # Load all cached daily basic for this stock and filter by date\n",
    "        cached_df = self._load_from_cache(file_path, '19900101', '20991231') # Load all to check specific date\n",
    "        if cached_df is not None and not cached_df.empty:\n",
    "            specific_date_df = cached_df[cached_df['trade_date'] == trade_date]\n",
    "            if not specific_date_df.empty:\n",
    "                # print(f\"Loaded daily basic for {ts_code} on {date_str} from cache.\")\n",
    "                return specific_date_df\n",
    "        \n",
    "        # print(f\"Fetching daily basic for {ts_code} on {date_str} from Tushare...\")\n",
    "        df = self.pro.daily_basic(ts_code=ts_code, trade_date=date_str, fields='ts_code,trade_date,pe_ttm,pb')\n",
    "        if df is None or df.empty:\n",
    "            # print(f\"No daily basic data fetched for {ts_code} on {date_str}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df['trade_date'] = pd.to_datetime(df['trade_date'])\n",
    "        # Append to existing cache or create new\n",
    "        if cached_df is not None and not cached_df.empty:\n",
    "            combined_df = pd.concat([cached_df, df]).drop_duplicates(subset=['ts_code', 'trade_date']).reset_index(drop=True)\n",
    "            self._save_to_cache(combined_df, file_path)\n",
    "        else:\n",
    "            self._save_to_cache(df, file_path)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2ef289c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2051489982.py, line 48)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"D:\\tmp\\ipykernel_29108\\2051489982.py\"\u001b[1;36m, line \u001b[1;32m48\u001b[0m\n\u001b[1;33m    pe_values =\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# factor_calculator.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_manager import TushareDataManager # Assuming data_manager.py is in the same directory\n",
    "\n",
    "class FactorCalculator:\n",
    "    def __init__(self, dm: TushareDataManager):\n",
    "        self.dm = dm\n",
    "        self.csi100_code = '000903.SH'\n",
    "        self.csi500_code = '000905.SH'\n",
    "\n",
    "    def calculate_returns(self, df, period):\n",
    "        \"\"\"计算指定周期内的收益率\"\"\"\n",
    "        return df['close'].pct_change(periods=period)\n",
    "\n",
    "    def calculate_rsi(self, df, period):\n",
    "        \"\"\"计算RSI\"\"\"\n",
    "        delta = df['close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "\n",
    "        rs = gain / loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi\n",
    "\n",
    "    def calculate_psy(self, df, period):\n",
    "        \"\"\"计算PSY (心理线)\"\"\"\n",
    "        # Count up weeks (close > prev_close)\n",
    "        up_weeks = (df['close'].diff() > 0).astype(int).rolling(window=period).sum()\n",
    "        psy = (up_weeks / period) * 100\n",
    "        return psy\n",
    "\n",
    "    def calculate_volume_ratio(self, df_csi500, df_csi100, period):\n",
    "        \"\"\"计算成交量比值\"\"\"\n",
    "        vol_csi500 = df_csi500['vol'].rolling(window=period).sum()\n",
    "        vol_csi100 = df_csi100['vol'].rolling(window=period).sum()\n",
    "        return vol_csi500 / vol_csi100\n",
    "\n",
    "    def calculate_median_pe_pb(self, index_code, trade_date):\n",
    "        \"\"\"\n",
    "        计算指定指数在特定日期的成分股PE/PB中值。\n",
    "        忽略PE为负或缺失的值。\n",
    "        \"\"\"\n",
    "        constituents_df = self.dm.get_index_constituents(index_code, trade_date)\n",
    "        if constituents_df.empty:\n",
    "            return np.nan, np.nan\n",
    "\n",
    "        pe_values =\n",
    "        pb_values =\n",
    "\n",
    "        # Filter constituents for the specific trade_date if multiple dates are returned\n",
    "        constituents_on_date = constituents_df[constituents_df['trade_date'] == trade_date]\n",
    "        if constituents_on_date.empty: # Fallback to latest available if exact date not found\n",
    "            latest_date = constituents_df['trade_date'].max()\n",
    "            constituents_on_date = constituents_df[constituents_df['trade_date'] == latest_date]\n",
    "            if constituents_on_date.empty:\n",
    "                return np.nan, np.nan\n",
    "\n",
    "        for _, row in constituents_on_date.iterrows():\n",
    "            stock_code = row['con_code']\n",
    "            daily_basic_df = self.dm.get_daily_basic(stock_code, trade_date)\n",
    "            if not daily_basic_df.empty:\n",
    "                pe = daily_basic_df['pe_ttm'].iloc\n",
    "                pb = daily_basic_df['pb'].iloc\n",
    "                \n",
    "                if pd.notna(pe) and pe > 0: # Ignore negative or NaN PE\n",
    "                    pe_values.append(pe)\n",
    "                if pd.notna(pb): # PB can be negative, but usually positive. Let's include all non-NaN.\n",
    "                    pb_values.append(pb)\n",
    "        \n",
    "        median_pe = np.median(pe_values) if pe_values else np.nan\n",
    "        median_pb = np.median(pb_values) if pb_values else np.nan\n",
    "        \n",
    "        return median_pe, median_pb\n",
    "\n",
    "    def calculate_all_factors(self, start_date, end_date):\n",
    "        \"\"\"\n",
    "        整合所有因子计算，生成包含所有因子值的DataFrame。\n",
    "        报告中“月”周期转换为周数：1月=4周, 3月=12周, 6月=24周, 12月=48周。\n",
    "        \"\"\"\n",
    "        print(\"Calculating all factors...\")\n",
    "        df_csi100 = self.dm.get_index_weekly_data(self.csi100_code, start_date, end_date)\n",
    "        df_csi500 = self.dm.get_index_weekly_data(self.csi500_code, start_date, end_date)\n",
    "\n",
    "        if df_csi100.empty or df_csi500.empty:\n",
    "            print(\"Error: Could not retrieve index weekly data.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Merge dataframes on trade_date\n",
    "        df_merged = pd.merge(df_csi100[['trade_date', 'close', 'vol', 'amount']],\n",
    "                             df_csi500[['trade_date', 'close', 'vol', 'amount']],\n",
    "                             on='trade_date', suffixes=('_csi100', '_csi500'))\n",
    "        \n",
    "        df_merged = df_merged.sort_values('trade_date').reset_index(drop=True)\n",
    "\n",
    "        # Calculate factors\n",
    "        # Returns difference (涨幅差值)\n",
    "        df_merged['ret_diff_4w'] = self.calculate_returns(df_merged, 4) - self.calculate_returns(df_merged, 4).shift(periods=4) # This is not correct based on report.\n",
    "        # Corrected: CSI500 return - CSI100 return\n",
    "        df_merged['ret_csi500_4w'] = self.calculate_returns(df_merged.rename(columns={'close_csi500': 'close'}), 4)\n",
    "        df_merged['ret_csi100_4w'] = self.calculate_returns(df_merged.rename(columns={'close_csi100': 'close'}), 4)\n",
    "        df_merged['ret_diff_4w'] = df_merged['ret_csi500_4w'] - df_merged['ret_csi100_4w']\n",
    "\n",
    "        df_merged['ret_csi500_12w'] = self.calculate_returns(df_merged.rename(columns={'close_csi500': 'close'}), 12)\n",
    "        df_merged['ret_csi100_12w'] = self.calculate_returns(df_merged.rename(columns={'close_csi100': 'close'}), 12)\n",
    "        df_merged['ret_diff_12w'] = df_merged['ret_csi500_12w'] - df_merged['ret_csi100_12w']\n",
    "\n",
    "        df_merged['ret_csi500_24w'] = self.calculate_returns(df_merged.rename(columns={'close_csi500': 'close'}), 24)\n",
    "        df_merged['ret_csi100_24w'] = self.calculate_returns(df_merged.rename(columns={'close_csi100': 'close'}), 24)\n",
    "        df_merged['ret_diff_24w'] = df_merged['ret_csi500_24w'] - df_merged['ret_csi100_24w']\n",
    "\n",
    "        df_merged['ret_csi500_48w'] = self.calculate_returns(df_merged.rename(columns={'close_csi500': 'close'}), 48)\n",
    "        df_merged['ret_csi100_48w'] = self.calculate_returns(df_merged.rename(columns={'close_csi100': 'close'}), 48)\n",
    "        df_merged['ret_diff_48w'] = df_merged['ret_csi500_48w'] - df_merged['ret_csi100_48w']\n",
    "\n",
    "        # RSI difference (RSI差值)\n",
    "        df_merged['rsi_csi500_4w'] = self.calculate_rsi(df_merged.rename(columns={'close_csi500': 'close'}), 4)\n",
    "        df_merged['rsi_csi100_4w'] = self.calculate_rsi(df_merged.rename(columns={'close_csi100': 'close'}), 4)\n",
    "        df_merged['rsi_diff_4w'] = df_merged['rsi_csi500_4w'] - df_merged['rsi_csi100_4w']\n",
    "\n",
    "        df_merged['rsi_csi500_12w'] = self.calculate_rsi(df_merged.rename(columns={'close_csi500': 'close'}), 12)\n",
    "        df_merged['rsi_csi100_12w'] = self.calculate_rsi(df_merged.rename(columns={'close_csi100': 'close'}), 12)\n",
    "        df_merged['rsi_diff_12w'] = df_merged['rsi_csi500_12w'] - df_merged['rsi_csi100_12w']\n",
    "\n",
    "        df_merged['rsi_csi500_24w'] = self.calculate_rsi(df_merged.rename(columns={'close_csi500': 'close'}), 24)\n",
    "        df_merged['rsi_csi100_24w'] = self.calculate_rsi(df_merged.rename(columns={'close_csi100': 'close'}), 24)\n",
    "        df_merged['rsi_diff_24w'] = df_merged['rsi_csi500_24w'] - df_merged['rsi_csi100_24w']\n",
    "\n",
    "        df_merged['rsi_csi500_48w'] = self.calculate_rsi(df_merged.rename(columns={'close_csi500': 'close'}), 48)\n",
    "        df_merged['rsi_csi100_48w'] = self.calculate_rsi(df_merged.rename(columns={'close_csi100': 'close'}), 48)\n",
    "        df_merged['rsi_diff_48w'] = df_merged['rsi_csi500_48w'] - df_merged['rsi_csi100_48w']\n",
    "\n",
    "        # PSY difference (PSY差值)\n",
    "        df_merged['psy_csi500_4w'] = self.calculate_psy(df_merged.rename(columns={'close_csi500': 'close'}), 4)\n",
    "        df_merged['psy_csi100_4w'] = self.calculate_psy(df_merged.rename(columns={'close_csi100': 'close'}), 4)\n",
    "        df_merged['psy_diff_4w'] = df_merged['psy_csi500_4w'] - df_merged['psy_csi100_4w']\n",
    "\n",
    "        df_merged['psy_csi500_12w'] = self.calculate_psy(df_merged.rename(columns={'close_csi500': 'close'}), 12)\n",
    "        df_merged['psy_csi100_12w'] = self.calculate_psy(df_merged.rename(columns={'close_csi100': 'close'}), 12)\n",
    "        df_merged['psy_diff_12w'] = df_merged['psy_csi500_12w'] - df_merged['psy_csi100_12w']\n",
    "\n",
    "        df_merged['psy_csi500_24w'] = self.calculate_psy(df_merged.rename(columns={'close_csi500': 'close'}), 24)\n",
    "        df_merged['psy_csi100_24w'] = self.calculate_psy(df_merged.rename(columns={'close_csi100': 'close'}), 24)\n",
    "        df_merged['psy_diff_24w'] = df_merged['psy_csi500_24w'] - df_merged['psy_csi100_24w']\n",
    "\n",
    "        df_merged['psy_csi500_48w'] = self.calculate_psy(df_merged.rename(columns={'close_csi500': 'close'}), 48)\n",
    "        df_merged['psy_csi100_48w'] = self.calculate_psy(df_merged.rename(columns={'close_csi100': 'close'}), 48)\n",
    "        df_merged['psy_diff_48w'] = df_merged['psy_csi500_48w'] - df_merged['psy_csi100_48w']\n",
    "\n",
    "        # Volume ratio (成交量比值)\n",
    "        df_merged['vol_ratio_4w'] = self.calculate_volume_ratio(\n",
    "            df_merged.rename(columns={'vol_csi500': 'vol'}),\n",
    "            df_merged.rename(columns={'vol_csi100': 'vol'}),\n",
    "            4\n",
    "        )\n",
    "        df_merged['vol_ratio_12w'] = self.calculate_volume_ratio(\n",
    "            df_merged.rename(columns={'vol_csi500': 'vol'}),\n",
    "            df_merged.rename(columns={'vol_csi100': 'vol'}),\n",
    "            12\n",
    "        )\n",
    "\n",
    "        # PE/PB difference (PE/PB差值) - This needs to be calculated per week\n",
    "        # This part is computationally intensive due to individual stock data fetching\n",
    "        pe_diffs =\n",
    "        pb_diffs =\n",
    "        for i, row in df_merged.iterrows():\n",
    "            trade_date = row['trade_date']\n",
    "            median_pe_csi500, median_pb_csi500 = self.calculate_median_pe_pb(self.csi500_code, trade_date)\n",
    "            median_pe_csi100, median_pb_csi100 = self.calculate_median_pe_pb(self.csi100_code, trade_date)\n",
    "            \n",
    "            pe_diffs.append(median_pe_csi500 - median_pe_csi100 if pd.notna(median_pe_csi500) and pd.notna(median_pe_csi100) else np.nan)\n",
    "            pb_diffs.append(median_pb_csi500 - median_pb_csi100 if pd.notna(median_pb_csi500) and pd.notna(median_pb_csi100) else np.nan)\n",
    "        \n",
    "        df_merged['pe_diff'] = pe_diffs\n",
    "        df_merged['pb_diff'] = pb_diffs\n",
    "\n",
    "        # Calculate next week's relative return for labeling\n",
    "        df_merged['next_week_csi500_return'] = df_merged['close_csi500'].pct_change(periods=-1).shift(1) # Shift to align with current week's factors\n",
    "        df_merged['next_week_csi100_return'] = df_merged['close_csi100'].pct_change(periods=-1).shift(1)\n",
    "        df_merged['next_week_relative_return'] = df_merged['next_week_csi500_return'] - df_merged['next_week_csi100_return']\n",
    "        \n",
    "        # Label: 1 if CSI500 outperforms CSI100, 0 otherwise\n",
    "        df_merged['label'] = (df_merged['next_week_relative_return'] > 0).astype(int)\n",
    "\n",
    "        # Drop rows with NaN in factors (due to rolling window or initial PE/PB calculation)\n",
    "        # The report uses '3月RSI&6月PSY&12月PSY&1月成交' for Model II\n",
    "        # So, we need 'rsi_diff_12w', 'psy_diff_24w', 'psy_diff_48w', 'vol_ratio_4w'\n",
    "        # And 'pe_diff' if it's part of the model (Model II does not explicitly list PE, but it's a key factor in the report)\n",
    "        # Let's include all factors for now and select later in strategy_logic\n",
    "        factor_columns = [\n",
    "            'rsi_diff_12w', 'psy_diff_24w', 'psy_diff_48w', 'vol_ratio_4w', # Model II factors\n",
    "            'ret_diff_12w', 'ret_diff_24w', # Other important return diffs\n",
    "            'pe_diff', 'pb_diff', # Valuation factors\n",
    "            'rsi_diff_4w', 'rsi_diff_24w', 'rsi_diff_48w',\n",
    "            'psy_diff_4w', 'psy_diff_12w',\n",
    "            'vol_ratio_12w'\n",
    "        ]\n",
    "        \n",
    "        # Ensure all required columns exist before dropping NaNs\n",
    "        df_merged = df_merged.dropna(subset=factor_columns + ['label']).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Factor calculation complete. Total rows with factors: {len(df_merged)}\")\n",
    "        return df_merged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
